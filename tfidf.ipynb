{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob as tb\n",
    "import math\n",
    "import io\n",
    "import codecs\n",
    "from __future__ import division\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "os.chdir('/users/asheets/Documents/Work_Computer_new/Work_Computer/Grad_School/PREDICT_453/Notebooks/DSI/')\n",
    "\n",
    "all_docs = []\n",
    "blob_list = []\n",
    "d = 24\n",
    "\n",
    "cachedStopWords = nltk.corpus.stopwords.words('english')\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(cachedStopWords) + r')\\b\\s*')\n",
    "\n",
    "for i in range(16):\n",
    "    doc_name = 'DSI' + str(d) + '.txt'\n",
    "    try:\n",
    "        with open(doc_name, 'r') as f:\n",
    "            sample = f.read()\n",
    "        sample = sample.decode('utf-8')\n",
    "       #sample = sample.decode('ascii')\n",
    "        sample = sample.lower()\n",
    "        sample = re.sub(r'[^\\w]', ' ', sample)\n",
    "        sample = ''.join([i for i in sample if not i.isdigit()])\n",
    "        sample = pattern.sub('', sample)\n",
    "        sample = \"\".join(l for l in sample if l not in string.punctuation)\n",
    "        sample2 = \" \".join(k for k in tb(sample).noun_phrases)\n",
    "        all_docs.append({'DSInum': d, 'raw_text': sample, 'noun_phrases_only':\n",
    "        sample2})\n",
    "        blob_list.append(tb(sample2))\n",
    "        d = d + 1\n",
    "    except IOError:\n",
    "        d = d + 1\n",
    "        pass\n",
    "    \n",
    "all_documents = pd.DataFrame(all_docs)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/\n",
    "\n",
    "#computes \"term frequency\" which is the number of times a word appears in a document blob, \n",
    "#normalized by dividing by the total number of words in blob. \n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "#returns the number of documents containing word. A generator expression is passed to the sum() function.\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "#computes \"inverse document frequency\" which measures how common a word is among all documents in bloblist. \n",
    "#The more common a word is, the lower its idf. \n",
    "#We take the ratio of the total number of documents to the number of documents containing word, \n",
    "#then take the log of that. Add 1 to the divisor to prevent division by zero.\n",
    "def idf(word, bloblist):\n",
    "     return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "#computes the TF-IDF score. It is simply the product of  tf and idf.\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#compare all three articles\n",
    "blob1 = tb(sample1)\n",
    "blob2 = blob1.noun_phrases\n",
    "term_freq = [tf(word,blob1) for word in blob1.words]\n",
    "tf_df1 = pd.DataFrame({'word': blob1.words, 'doc1_term_freq': term_freq})\n",
    "\n",
    "blob2 = tb(sample2)\n",
    "term_freq = [tf(word,blob2) for word in blob2.words]\n",
    "tf_df2 = pd.DataFrame({'word': blob2.words, 'doc2_term_freq': term_freq})\n",
    "\n",
    "blob3 = tb(sample3)\n",
    "term_freq = [tf(word,blob3) for word in blob3.words]\n",
    "tf_df3 = pd.DataFrame({'word': blob3.words, 'doc3_term_freq': term_freq})\n",
    "\n",
    "all_tf = pd.merge(pd.merge(tf_df1,tf_df2,on='word',how='outer'),tf_df3,on='word',how='outer').drop_duplicates()\n",
    "\n",
    "bloblist = [blob1,blob2,blob3]\n",
    "\n",
    "docs_containing1 = pd.DataFrame({'word': blob1.words, 'doc_freq': [n_containing(word,bloblist) for word in blob1.words]})\n",
    "docs_containing2 = pd.DataFrame({'word': blob2.words, 'doc_freq': [n_containing(word,bloblist) for word in blob2.words]})\n",
    "docs_containing3 = pd.DataFrame({'word': blob3.words, 'doc_freq': [n_containing(word,bloblist) for word in blob3.words]})\n",
    "\n",
    "doc_freq = pd.concat([docs_containing1,docs_containing2,docs_containing3]).drop_duplicates().reset_index(drop=True)\n",
    "df = pd.merge(doc_freq,all_tf,on='word',how='inner')\n",
    "df = df.sort_values(\"doc_freq\",ascending=False)\n",
    "\n",
    "df['intermediate_calc'] = (len(bloblist) / df['doc_freq']).astype(float)\n",
    "df['idf'] = df['intermediate_calc'].apply(math.log)\n",
    "df = df.sort_values(\"idf\",ascending=True)\n",
    "\n",
    "\n",
    "scores1 = pd.DataFrame({'word': blob1.words, 'tf_idf_doc1': [tfidf(word, blob1, bloblist) for word in blob1.words]})\n",
    "scores2 = pd.DataFrame({'word': blob2.words, 'tf_idf_doc2': [tfidf(word, blob2, bloblist) for word in blob2.words]})\n",
    "scores3 = pd.DataFrame({'word': blob3.words, 'tf_idf_doc3': [tfidf(word, blob3, bloblist) for word in blob3.words]})\n",
    "\n",
    "all_tfidf= pd.merge(pd.merge(scores1,scores2,on='word',how='outer'),scores3,on='word',how='outer').drop_duplicates()\n",
    "all_tfidf = all_tfidf[['word', 'tf_idf_doc1', 'tf_idf_doc2', 'tf_idf_doc3']]\n",
    "all_tfidf = all_tfidf.sort_values(by=['tf_idf_doc1', 'tf_idf_doc2','tf_idf_doc3'], ascending=[False,False,False])\n",
    "all_tfidf.head(n=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##Compare just the two articles we know to be similar\n",
    "two_tf = pd.merge(tf_df1,tf_df2,on='word',how='outer').drop_duplicates()\n",
    "\n",
    "bloblist = [blob1,blob2]\n",
    "\n",
    "docs_containing1 = pd.DataFrame({'word': blob1.words, 'doc_freq': [n_containing(word,bloblist) for word in blob1.words]})\n",
    "docs_containing2 = pd.DataFrame({'word': blob2.words, 'doc_freq': [n_containing(word,bloblist) for word in blob2.words]})\n",
    "\n",
    "doc_freq = pd.concat([docs_containing1,docs_containing2]).drop_duplicates().reset_index(drop=True)\n",
    "df = pd.merge(doc_freq,two_tf,on='word',how='inner')\n",
    "df = df.sort_values(\"doc_freq\",ascending=False)\n",
    "\n",
    "df['intermediate_calc'] = (len(bloblist) / df['doc_freq']).astype(float)\n",
    "df['idf'] = df['intermediate_calc'].apply(math.log)\n",
    "df = df.sort_values(\"idf\",ascending=False)\n",
    "\n",
    "scores1 = pd.DataFrame({'word': blob1.words, 'tf_idf_doc1': [tfidf(word, blob1, bloblist) for word in blob1.words]})\n",
    "scores2 = pd.DataFrame({'word': blob2.words, 'tf_idf_doc2': [tfidf(word, blob2, bloblist) for word in blob2.words]})\n",
    "\n",
    "two_tfidf= pd.merge(scores1,scores2,on='word',how='outer').drop_duplicates()\n",
    "two_tfidf = two_tfidf[['word', 'tf_idf_doc1', 'tf_idf_doc2']]\n",
    "two_tfidf = two_tfidf.sort_values(by=['tf_idf_doc1', 'tf_idf_doc2'], ascending=[False,False])\n",
    "two_tfidf.head(n=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compare all three using all pre-defined functions\n",
    "bloblist2 = [blob1,blob2,blob3]\n",
    "for i, blob in enumerate(bloblist2):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:10]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#compare similar docs using all pre-defined functions\n",
    "bloblist3 = [blob1,blob2]\n",
    "for i, blob in enumerate(bloblist3):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:10]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#compare all DSIs sing all pre-defined functions\n",
    "DSI_list = all_documents[\"DSInum\"]\n",
    "for i, blob in enumerate(blob_list):\n",
    "    print(\"Top words in document {}\".format(DSI_list[i]))\n",
    "    scores = {word: tfidf(word, blob, blob_list) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:10]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RTV = pd.read_csv('RTV.txt',header=None)\n",
    "RTV.columns = ['word']\n",
    "\n",
    "RTVblob = tb(str(tuple(RTV.word.tolist())).replace(\"'\", \"\"))\n",
    "\n",
    "tf_list = []\n",
    "for i, blob in enumerate(blob_list):\n",
    "    for word in RTVblob.words:\n",
    "        tf_list.append({'DSInum': DSI_list[i], 'word': word, 'term_freq': blob.words.count(word)})\n",
    "\n",
    "#print pd.DataFrame(tf_list)\n",
    "tf_df = pd.DataFrame(tf_list)\n",
    "tf_df.to_csv(\"RTV_frequencies.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
